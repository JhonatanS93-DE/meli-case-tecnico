# Mercado Libre - Senior Data Engineer Case

## ğŸ“ DescripciÃ³n del Caso

Este proyecto corresponde a la resoluciÃ³n de un ejercicio tÃ©cnico para la posiciÃ³n de **Senior Data Engineer** en Mercado Libre. El objetivo es construir un pipeline de datos que procese mÃºltiples fuentes, genere un dataset enriquecido para un modelo de machine learning y lo almacene en diferentes formatos y destinos.

---

## ğŸš€ Objetivos del Pipeline

- Leer fuentes de datos en formato JSONL y CSV (`prints.json`, `taps.json`, `pays.csv`).
- Filtrar eventos de la Ãºltima semana.
- Calcular mÃ©tricas agregadas para las tres semanas previas:
  - NÃºmero de vistas (`prints`)
  - NÃºmero de clics (`taps`)
  - NÃºmero y monto de pagos (`pays`)
- Enriquecer los datos con estas mÃ©tricas.
- Validar la calidad del dataset resultante.
- Guardar el dataset final en:
  - CSV
  - Parquet
  - PostgreSQL

---

## ğŸ› ï¸ TecnologÃ­as y Herramientas

- **Python 3.10+**
- **Apache Airflow** (vÃ­a Docker Compose)
- **PostgreSQL** (DBeaver como cliente de consulta)
- **Pandas / PyArrow**
- **Docker + Docker Compose**

---

## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                # DAG principal de Airflow
â”‚   â””â”€â”€ docker/              # Dockerfile y dependencias
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/               # Archivos fuente
â”‚   â””â”€â”€ output/              # Salidas CSV / Parquet
â”œâ”€â”€ notebooks/               # AnÃ¡lisis exploratorio
â”œâ”€â”€ pipeline/                # CÃ³digo modular de ETL
â”œâ”€â”€ sql/                     # Scripts SQL (creaciÃ³n de tablas)
â”œâ”€â”€ .env                     # Variables de entorno (opcional)
â”œâ”€â”€ docker-compose.yml       # OrquestaciÃ³n de servicios
â””â”€â”€ README.md
```

---

## âš™ï¸ CÃ³mo Ejecutar el Proyecto

1. Clona este repositorio:
```bash
git clone https://github.com/JhonatanS93-DE/meli-case-tecnico.git
cd meli-case-tecnico
```

2. AsegÃºrate de tener instalado `Docker` y `Docker Compose`.

3. Ejecuta el entorno:
```bash
docker-compose up --build
```

4. Accede a Airflow:
```
http://localhost:8080
(user: admin / password: admin)
```

5. Sube tus archivos fuente a `data/input/` y ejecuta el DAG `mercado_libre_pipeline_dag`.

---

## âœ… Mejoras Futuras

- Tests automatizados con `pytest`.
- Validaciones de esquema con `Great Expectations`.
- ParametrizaciÃ³n por entorno.
- OrquestaciÃ³n con DAGs mÃ¡s robustos y modularizaciÃ³n por etapa.

---

## ğŸ§‘â€ğŸ’» Autor

**Jhonatan Saldarriaga**  
[LinkedIn](https://www.linkedin.com/in/jhonatan-saldarriaga/)  
Senior Data Engineer